# 设计 Twitter

## 设计 Twitter

让我们设计一个类似 Twitter 的社交网络服务。该服务的用户将能够发布推文、关注其他人和喜爱的推文。

难度级别：中等



### 1、什么是Twitter？ <a href="#div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px1-what-is-twitter" id="div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px1-what-is-twitter"></a>

Twitter 是一种在线社交网络服务，用户可以在其中发布和阅读称为“推文”的 140 个字符的简短消息。注册用户可以发布和阅读推文，但未注册的用户只能阅读。用户通过他们的网站界面、SMS 或移动应用程序访问 Twitter。



### 2、系统的要求和目标

我们将设计一个具有以下要求的更简单的 Twitter 版本：

#### 功能要求

1. 用户应该能够发布新的推文。
2. 用户应该能够关注其他用户。
3. 用户应该能够将推文标记为收藏。
4. 该服务应该能够创建和显示用户的时间线，该时间线由用户关注的所有人的热门推文组成。
5. 推文可以包含照片和视频。

#### 非功能性需求

1. 我们的服务需要高可用。
2. 对于时间线生成，系统可接受的延迟为 200 毫秒。
3. 如果用户一段时间没有到读到推文，一致性收到影响（为了可用性），这是可以接受的。

**扩展要求**

1. 搜索推文。
2. 回复一条推文。
3. 热门话题——当前热门话题/搜索。
4. 标记其他用户。
5. 推文通知。
6. 关注谁？建议？
7. 朋友圈。

### 3、容量估计和约束

假设我们有 10 亿总用户和 2 亿每日活跃用户 (DAU)。还假设我们每天有 1 亿条新推文，平均每个用户关注 200 人。

**每天有多少个收藏？**如果平均而言，每个用户每天收藏 5 条推文，我们将拥有：

```
200M 用户 * 5 个收藏 => 1 B(billion)收藏
```

**我们的系统将产生多少总推文浏览量？**假设用户平均每天访问他们的时间线两次并访问其他五个人的页面。如果用户在每个页面上看到 20 条推文，那么我们的系统将生成 28B/天的总推文浏览量：

```
200M DAU * ((2 + 5) * 20 条推文) => 28B/天
```

**存储估计**假设每条推文有 140 个字符，我们需要两个字节来存储一个字符而不进行压缩。假设我们需要 30 个字节来存储每条推文的元数据（如 ID、时间戳、用户 ID 等）。我们需要的总存储空间：

```
100M * (280 + 30) 字节 => 30GB/天
```

我们五年的存储需求是多少？我们需要多少存储空间来存储用户的数据、关注和收藏？我们将把它留给练习。

并非所有推文都有媒体，我们假设平均每 5 条推文有一张照片，每 10 条推文都有一段视频。我们还假设平均一张照片是 200KB，一个视频是 2MB。这将导致我们每天拥有 24TB 的新媒体。

```
(100M/5 张照片 * 200KB) + (100M/10 个视频 * 2MB) ~= 24TB/天
```

**带宽估计** 由于每天的总入口为 24TB，这将转化为 290MB/秒。

请记住，我们每天有 28B 的推文浏览量。我们必须展示每条推文的照片（如果它有照片），但让我们假设用户观看他们在时间轴中看到的每个第三个视频。因此，总出口将是：

```
(28B * 280 字节) / 86400s 文本 => 93MB/s
+ (28B/5 * 200KB ) / 86400s 照片 => 13GB/S
+ (28B/10/3 * 2MB) / 86400s 视频 => 22GB/ s
总计 ~= 35GB/s
```

### 4、系统API

> 💡 一旦我们最终确定了需求，定义系统 API 总是一个好主意。这应该明确说明对系统的期望。

我们可以使用 SOAP 或 REST API 来公开我们服务的功能。以下可能是发布新推文的 API 定义：

```
tweet(api_dev_key, tweet_data, tweet_location, user_location, media_ids)
```

**参数：**\
api\_dev\_key(string)：注册账号的API开发者密钥。除其他外，这将用于根据分配的配额限制用户。\
tweet\_data (string)：推文的文本，通常最多 140 个字符。\
tweet\_location（字符串）：此推文所指的可选位置（经度、纬度）。\
user\_location（字符串）：添加推文的用户的可选位置（经度、纬度）。\
media\_ids (number\[])：与推文关联的可选 media\_id 列表。（所有媒体照片、视频等需要单独上传）。

**返回：（**字符串）\
成功的帖子将返回访问该推文的 URL。否则，将返回适当的 HTTP 错误。



### 5、高层次系统设计

我们需要一个能够有效存储所有新推文的系统，每秒 100M/86400s => 1150 条推文，每秒读取 28B/86400s => 325K 条推文。从需求中可以清楚地看出，这将是一个读取繁重的系统。

在高层次上，我们需要多个应用程序服务器来处理所有这些请求，并在它们前面使用负载均衡器来分配流量。在后端，我们需要一个高效的数据库来存储所有新推文并支持大量读取。我们还需要一些文件存储来存储照片和视频。

\
\


<figure><img src="../.gitbook/assets/image (3) (2).png" alt=""><figcaption><p>高层次系统设计</p></figcaption></figure>

尽管我们预期的每日写入负载为 1 亿条，读取负载为 280 亿条推文。这意味着我们的系统平均每秒将收到大约 1160 条新推文和 325K 读取请求。该流量将在一天中不均匀地分布，但是，在高峰时间我们应该预计每秒至少有几千个写入请求和大约 1M 的读取请求。在设计系统架构时，我们应该牢记这一点。



### 6、数据库模式

我们需要存储有关用户、他们的推文、他们最喜欢的推文以及他们关注的人的数据。\


<figure><img src="../.gitbook/assets/image (5) (2).png" alt=""><figcaption><p>数据库模式</p></figcaption></figure>

要在 SQL 和 NoSQL 数据库之间选择存储上述模式，请参阅[设计 Instagram](she-ji-instagram.md)下的“数据库模式” 。



### 7、数据分片 <a href="#div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px7-data-sharding" id="div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px7-data-sharding"></a>

由于我们每天都有大量的新推文，而且我们的读取负载也非常高，我们需要将数据分布到多台机器上，以便我们可以高效地读取/写入它。我们有很多选择来分片我们的数据；让我们一一介绍：

**基于 UserID 的分片**

我们可以尝试将用户的所有数据存储在一台服务器上。在存储时，我们可以将 UserID 传递给我们的哈希函数，该函数将用户映射到数据库服务器，我们将在其中存储用户的所有推文、收藏夹、关注等。在查询用户的推文/关注/收藏时，我们可以询问我们的哈希函数在哪里可以找到用户的数据，然后从那里读取。这种方法有几个问题：

1. 如果用户变热了怎么办？持有用户的服务器上可能有很多查询。这种高负载会影响我们服务的性能。
2. 随着时间的推移，一些用户最终可能会存储大量推文或与其他用户相比拥有大量关注。保持不断增长的用户数据的均匀分布是相当困难的。\


要从这些情况中恢复，我们必须重新分区/重新分配我们的数据或使用一致的哈希。 ****&#x20;

#### **基于 TweetID 的分片**

我们的哈希函数会将每个 TweetID 映射到一个随机服务器，我们将在其中存储该 Tweet。要搜索推文，我们必须查询所有服务器，每个服务器都会返回一组推文。中央服务器将汇总这些结果以将它们返回给用户。让我们看一下时间线生成示例；以下是我们的系统为生成用户的时间线而必须执行的步骤数：

1. 我们的应用（app）服务器会找到用户关注的所有人。
2. 应用服务器会将查询发送到所有数据库服务器以查找来自这些人的推文。
3. 每个数据库服务器都会找到每个用户的推文，按时间对它们进行排序并返回排名靠前的推文。
4. 应用服务器会将所有结果合并并重新排序，将排名靠前的结果返回给用户。

这种方法解决了热用户的问题，但是，与通过 UserID 分片相比，我们必须查询所有数据库分区才能找到用户的推文，这会导致更高的延迟。

我们可以通过在数据库服务器前引入缓存来存储热门推文来进一步提高我们的性能。

#### **基于推文创建时间的分片**

基于创建时间存储推文将使我们具有快速获取所有热门推文的优势，并且我们只需要查询非常小的一组服务器。这里的问题是流量负载不会被分配，例如，在写作时，所有新推文都将发送到一台服务器，而其余服务器将处于空闲状态。同样，在读取时，与保存旧数据的服务器相比，保存最新数据的服务器将具有非常高的负载。\
**如果我们可以结合 TweetID 和 Tweet 创建时间的分片呢？**如果我们不单独存储推文创建时间并使用 TweetID 来反映这一点，我们可以获得这两种方法的好处。这样可以很快找到最新的推文。为此，我们必须使每个 TweetID 在我们的系统中普遍唯一，并且每个 TweetID 也应该包含一个时间戳。

我们可以为此使用纪元时间。假设我们的 TweetID 将有两部分：第一部分将表示纪元秒数，第二部分将是一个自动递增的序列。因此，要创建一个新的 TweetID，我们可以采用当前纪元时间并为其附加一个自动递增的数字。我们可以从这个 TweetID 中找出分片号并将其存储在那里。

我们的 TweetID 的大小可能是多少？假设我们的纪元时间从今天开始，我们需要多少位来存储未来 50 年的秒数？

```
86400 秒/天 * 365（一年中的天数）* 50（年）=> 1.6B
```

\


<figure><img src="../.gitbook/assets/image (9) (2).png" alt=""><figcaption></figcaption></figure>

我们需要 31 位来存储这个数字。由于我们平均期望每秒有 1150 条新推文，我们可以分配 17 位来存储自动递增的序列；这将使我们的 TweetID 长 48 位。因此，我们每秒可以存储 (2^17 => 130K) 条新推文。我们可以每秒重置我们的自动递增序列。为了容错和更好的性能，我们可以有两台数据库服务器为我们生成自增键，一台生成偶数键，另一台生成奇数键。

如果我们假设我们当前的纪元秒数是“1483228800”，我们的 TweetID 将如下所示：

```
1483228800 000001
1483228800 000002
1483228800 000003
1483228800 000004
...
```

如果我们将 TweetID 设为 64 位（8 字节）长，我们​​可以轻松地存储未来 100 年的推文，并将它们存储为毫秒级。\


在上述方法中，我们仍然需要查询所有服务器以生成时间线，但我们的读取（和写入）将大大加快。

1. 由于我们没有任何二级索引（在创建时），这将减少我们的写入延迟。
2. 在读取时，我们不需要过滤创建时间，因为我们的主键包含了纪元时间。



### 8、缓存

我们可以为数据库服务器引入一个缓存来缓存热门推文和用户。我们可以使用像 Memcache 这样的现成解决方案，它可以存储整个推文对象。应用程序服务器在访问数据库之前，可以快速检查缓存是否有所需的推文。根据客户的使用模式，我们可以确定我们需要多少缓存服务器。

**哪种缓存替换策略最适合我们的需求？**当缓存已满并且我们想用更新/更热门的推文替换推文时，我们将如何选择？最近最少使用 (LRU) 可能是我们系统的合理策略。根据这项政策，我们首先丢弃最近最少查看的推文。

**我们如何才能拥有更智能的缓存？**如果我们采用 80-20 规则，即 20% 的推文产生 80% 的阅读流量，这意味着某些推文非常受欢迎，以至于大多数人都会阅读它们。这表明我们可以尝试从每个分片缓存 20% 的每日读取量。

**如果我们缓存最新的数据呢？**我们的服务可以从这种方法中受益。假设 80% 的用户只看到过去三天的推文；我们可以尝试缓存过去三天的所有推文。假设我们有专门的缓存服务器来缓存过去三天所有用户的所有推文。如上所述，我们每天收到 1 亿条新推文或 30GB 新数据（不含照片和视频）。如果我们想存储过去三天的所有推文，我们需要不到 100GB 的内存。这些数据可以轻松放入一台服务器，但我们应该将其复制到多台服务器上以分配所有读取流量以减少缓存服务器的负载。因此，每当我们生成用户的时间线时，我们都可以询问缓存服务器是否拥有该用户的所有最新推文。如果是，我们可以简单地从缓存中返回所有数据。如果缓存中没有足够的推文，我们必须查询后端服务器以获取该数据。在类似的设计中，我们可以尝试缓存过去三天的照片和视频。

我们的缓存就像一个哈希表，其中“key”是“OwnerID”，“value”是一个双向链表，其中包含该用户在过去三天内的所有推文。由于我们想首先检索最新的数据，我们总是可以在链表的头部插入新的推文，这意味着所有较旧的推文都将在链表的尾部附近。因此，我们可以从尾部删除推文，为更新的推文腾出空间。

<figure><img src="../.gitbook/assets/image (13) (2).png" alt=""><figcaption><p>缓存</p></figcaption></figure>

### 9、时间线生成

有关时间线生成的详细讨论，请查看[Designing Facebook's Newsfeed](https://www.educative.io/collection/page/5668639101419520/5649050225344512/5641332169113600)。



### 10、复制和容错 <a href="#div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px10-replication-and-fault-toler" id="div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px10-replication-and-fault-toler"></a>

由于我们的系统是读取繁重的，我们可以为每个 DB 分区拥有多个辅助数据库服务器。辅助服务器将仅用于读取流量。所有写入将首先转到主服务器，然后将被复制到辅助服务器。该方案还将为我们提供容错能力，因为每当主服务器出现故障时，我们都可以故障转移到辅助服务器。



### 11、负载均衡 <a href="#div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px11-load-balancing" id="div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px11-load-balancing"></a>

我们可以在系统中的三个地方添加负载均衡层：

1. _在客户端和应用服务器之间；_
2. 在聚合服务器和数据库复制服务器之间；
3. 在聚合服务器和高速缓存服务器之间。

最初，可以采用一个简单的Round Robin方法，将传入的请求在服务器之间平均分配。这种方法实现起来很简单，而且不会引入任何开销。这种方法的另一个好处是，如果一个服务器挂了，LB会把它从轮流中剔除，并停止向它发送任何流量。Round Robin LB的一个问题是，它不考虑服务器的负载。如果一个服务器过载或缓慢，LB不会停止向该服务器发送新请求。为了处理这个问题，可以放置一个更智能的LB解决方案，定期查询后端服务器的负载，并在此基础上调整流量。



### 12、监控 <a href="#div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px12-monitoring" id="div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px12-monitoring"></a>

能够监控我们的系统至关重要。我们应该不断收集数据，以便立即了解我们的系统是如何运行的。我们可以收集以下指标/计数器来了解我们服务的性能：

1. 每天/秒的新推文，每天的峰值是多少？
2. 时间线交付统计数据，我们的服务每天/秒交付多少条推文。
3. 用户看到的刷新时间线的平均延迟。

通过监控这些计数器，我们将了解是否需要更多的复制、负载平衡或缓存。



### 13、扩展要求 <a href="#div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px13-extended-requirements" id="div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px13-extended-requirements"></a>

**我们如何提供提要？** 从某人关注的人那里获取所有最新推文，并按时间合并/排序。使用分页来获取/显示推文。仅从某人关注的所有人中获取前 N 条推文。这个 N 将取决于客户端的 Viewport，因为与 Web 客户端相比，在移动设备上我们显示的推文更少。我们还可以缓存下一条热门推文以加快速度。

或者，我们可以预先生成以提高效率；有关详细信息，请参阅[设计 Instagram](she-ji-instagram.md)下的“排名和时间线生成” 。

**Retweet：**对于数据库中的每个 Tweet 对象，我们可以存储原始 Tweet 的 ID，而不在此 retweet 对象上存储任何内容。\
**热门话题：**我们可以缓存最近 N 秒内出现频率最高的主题标签或搜索查询，并在每 M 秒后不断更新它们。我们可以根据推文或搜索查询或转发或喜欢的频率对热门话题进行排名。我们可以更加重视向更多人展示的主题。

**关注谁？如何给出建议？**此功能将提高用户参与度。我们可以推荐某人关注的人的朋友。我们可以往下走两到三层，寻找名人的建议。我们可以优先考虑拥有更多关注者的人。

由于任何时候都只能提出一些建议，因此请使用机器学习 (ML) 来调整和重新确定优先级。ML 信号可能包括最近增加关注的人、如果其他人正在关注此用户、共同的位置或兴趣等的共同追随者。

**朋友圈：**获取过去 1 或 2 小时内不同网站的头条新闻，找出相关推文，对其进行优先排序，使用 ML（监督学习或聚类）对其进行分类（新闻、支持、财务、娱乐等）。然后我们可以将这些文章显示为朋友圈中的热门话题。

**搜索：**搜索涉及推文的索引、排名和检索。[在我们的下一个问题Design Twitter Search](https://www.educative.io/collection/page/5668639101419520/5649050225344512/5738600293466112)中讨论了类似的解决方案。

\
\
\


\


\
