# 设计 Instagram

## 设计Instagram

让我们设计一个像Instagram这样的照片分享服务，用户可以上传照片与其他用户分享。

类似服务：Flickr、Picasa

难度级别：中

### 1、什么是 Instagram？

Instagram 是一种社交网络服务，它的用户可以上传并与其他用户分享他们的照片和视频。Instagram 用户可以选择公开或私下共享信息。任何其他用户都可以看到公开共享的任何内容，而私人共享的内容只能由指定的一组人访问。Instagram 还允许其用户通过许多其他社交网络平台进行分享，例如 Facebook、Twitter、Flickr 和 Tumblr。

为了这个练习，我们计划设计一个更简单的 Instagram 版本，用户可以在其中分享照片，也可以关注其他用户。每个用户的“News Feed”将包含用户关注的所有人的热门照片。

### 2、系统的要求和目标

在设计 Instagram 时，我们将关注以下一组要求：

#### **功能要求**

1. 用户应该能够上传/下载/查看照片。
2. 用户可以根据照片/视频标题执行搜索。
3. 用户可以关注其他用户。
4. 系统应该能够生成和显示用户的News Feed，其中包含用户关注的所有人的热门照片。

#### **非功能性需求**

1. 我们的服务需要高可用。
2. 系统可接受的动态消息生成延迟为 200 毫秒。
3. 如果用户一段时间没有到读到照片，一致性收到影响（为了可用性），这是可以接受的。
4. 系统应该是高可靠的；任何上传的照片或视频都不应丢失。

**不在范围内：**为照片添加标签、在标签上搜索照片、评论照片、将用户标记到照片、关注谁等。

### 3、一些设计考虑

该系统将是重读取的，因此我们将专注于构建一个可以快速检索照片的系统。

1. 实际上，用户可以上传任意数量的照片。在设计这个系统时，有效的存储管理应该是一个关键因素。
2. 查看照片时需要低延迟。
3. 数据应该是 100% 可靠的。如果用户上传照片，系统将保证照片永远不会丢失。

### 4、 容量估计和约束

* 假设我们有 5 亿总用户，每天有 100 万活跃用户。
* 每天200万张新照片，每秒23张新照片。
* 平均照片文件大小 => 200KB
* 1 天照片所需的总空间

```
2M * 200KB => 400 GB
```

* 10年所需的总空间：

```
400GB * 365* 10~= 1425TB
```

### 5、 高层次系统设计（High level design ，HLD）

在高层次上，我们需要支持两种场景，一种是上传照片，另一种是查看/搜索照片。我们的服务需要一些[对象存储](https://en.wikipedia.org/wiki/Object\_storage)服务器来存储照片，还需要一些数据库服务器来存储有关照片的元数据信息。

<figure><img src="../.gitbook/assets/image (17).png" alt=""><figcaption><p>高层次系统设计（High level design ，HLD）</p></figcaption></figure>

### 6、数据库设计

> 💡 在面试的早起阶段定义数据库模式将有助于理解各个组件之间的数据流，然后将指导数据分区。

我们需要存储有关用户、他们上传的照片以及他们关注的人的数据。照片表将存储与照片相关的所有数据；我们需要在 (PhotoID, CreationDate) 上有一个索引，因为我们需要先获取最近的照片。

<figure><img src="../.gitbook/assets/image (13) (1).png" alt=""><figcaption><p>数据库设计</p></figcaption></figure>

存储上述模式的一种直接方法是使用像Mysql这样的RDBMS，因为我们需要连接（Join）。但是关系数据库也带来了挑战，尤其是当我们需要扩展它们时。有关详细信息，请查看[SQL与NOSQL](https://github.com/FangZzzzz/grokking\_system\_design/blob/master/System%20Design%20Problems)。

我们可以将照片存储在[HDFS](https://en.wikipedia.org/wiki/Apache\_Hadoop)或[S3](https://en.wikipedia.org/wiki/Amazon\_S3)等分布式文件存储中。

我们可以将上述模式存储在分布式键值存储中，以享受 NoSQL 提供的好处。所有与照片相关的元数据都可以放在一个表中，其中“键”是“PhotoID”，“值”是包含 PhotoLocation、UserLocation、CreationTimestamp 等的对象。

我们需要存储用户和照片之间的关系，以知道谁拥有哪张照片。我们还需要存储用户关注的人员列表。对于这两个表，我们可以使用像[Cassandra](https://en.wikipedia.org/wiki/Apache\_Cassandra)这样的宽列数据存储。对于“UserPhoto”表，“key”是“UserID”，“value”是用户拥有的“PhotoID”列表，存储在不同的列中。我们将为“UserFollow”表提供类似的方案。

通常，Cassandra 或键值存储始终维护一定数量的副本以提供可靠性。此外，在此类数据存储中，删除不会立即应用，数据会保留几天（以支持取消删除），然后再从系统中永久删除。

### 7、数据大小估计

让我们估计有多少数据将进入每张表，以及 10 年内我们需要多少总存储空间。

**用户：**假设每个“int”和“dateTime”是四个bytes，用户表中的每一行将是 68 个bytes：

{% code overflow="wrap" %}
```
UserID（4 bytes）+ 姓名（20 bytes）+ 电子邮件（32 bytes）+ DateOfBirth（4 bytes）+ CreationDate（4 bytes）+ LastLogin（4 bytes）= 68 bytes
```
{% endcode %}

如果我们有 5 亿用户，我们将需要 32GB 的总存储空间。

```
5亿*68~=32GB
```

**照片：**照片表中的每一行都是 284 bytes：

{% code overflow="wrap" %}
```
PhotoID (4 bytes) + UserID (4 bytes) + PhotoPath (256 bytes) + PhotoLatitude (4 bytes) + PhotLongitude (4 bytes) + UserLatitude (4 bytes) + UserLongitude (4 bytes) + CreationDate (4 bytes) = 284 bytes
```
{% endcode %}

如果每天上传 200 万张新照片，我们一天需要 0.5GB 的存储空间：

```
5 亿用户 * 500 个关注者 * 8 bytes ~= 1.82TB
```

10 年内所有表所需的总空间为 3.7TB：

```
32GB + 1.88TB + 1.82TB ~= 3.7TB
```

### 8、组件设计

照片上传（或写入）可能会很慢，因为它们必须进入磁盘，而读取会更快，特别是如果它们是从直接读取缓存的。

上传用户可以消耗所有可用的连接，因为上传是一个缓慢的过程。这意味着如果系统忙于所有写入请求，则无法提供“读取”服务。我们应该记住，在设计我们的系统之前，Web 服务器有连接限制。如果我们假设一个 Web 服务器在任何时候最多可以有 500 个连接，那么它不能有超过 500 个并发上传或读取。为了解决这个瓶颈，我们可以将读取和写入拆分为单独的服务。我们将有专门的读取服务器和不同的写入服务器，以确保上传不会占用系统。

分离照片的读取和写入请求还将允许我们独立地扩展和优化这些操作中的每一个。

<figure><img src="../.gitbook/assets/image (5) (1).png" alt=""><figcaption><p>组件设计</p></figcaption></figure>

### 9、 可靠性和冗余性

我们的服务不能丢失文件。因此，我们将存储每个文件的多个副本，以便如果一个存储服务器死机，我们可以从另一个存储服务器上的另一个副本中检索照片。

同样的原则也适用于系统的其他组件。如果我们想要系统的高可用性，我们需要在系统中运行多个服务副本，这样即使有一些服务宕机，系统仍然保持可用和运行。冗余消除了系统中的单点故障。

如果在任何时候只需要运行一个服务实例，我们可以运行该服务的冗余辅助副本，该副本不提供任何流量，但当主服务器出现问题时，它可以在故障转移后进行控制。

在系统中创建冗余可以消除单点故障，并在危机中需要时提供备份或备用功能。例如，如果同一服务的两个实例在生产中运行，其中一个发生故障或降级，则系统可以故障转移到健康副本。故障转移可以自动发生，也可以需要人工干预。

<figure><img src="../.gitbook/assets/image (14).png" alt=""><figcaption><p>可靠性和冗余性</p></figcaption></figure>

### 10、数据分片

让我们讨论元数据分片的不同方案：

#### a、基于 UserID的分区

假设我们基于“UserID”进行分片，这样我们就可以将用户的所有照片保存在同一个分片上。如果一个 DB shard 是 1TB，我们将需要四个 shard 来存储 3.7TB 的数据。假设为了更好的性能和可扩展性，我们保留 10 个分片。

所以我们将通过 UserID % 10 找到分片号，然后将数据存储在那里。为了唯一标识我们系统中的任何照片，我们可以为每个 PhotoID 附加分片号。

**我们如何生成 PhotoID？**每个 DB 分片都可以有自己的 PhotoID 自动递增序列，并且由于我们将为每个 PhotoID 附加 ShardID，它将使其在我们的系统中独一无二。

**这种分区方案有哪些不同的问题？**

1. 我们将如何处理热门用户？有几个人关注了这些热门用户，还有很多其他人看到了他们上传的任何照片。
2. 与其他用户相比，某些用户将拥有大量照片，从而导致存储分布不均匀。
3. 如果我们不能将用户的所有图片存储在一个分片上怎么办？如果我们将用户的照片分发到多个分片上会导致更高的延迟吗？
4. 将用户的所有照片存储在一个分片上可能会导致问题，例如，如果该分片已关闭，则所有用户的数据都将不可用，或者如果它正在服务于高负载，则会导致更高的延迟等。

#### b、**基于 PhotoID 的分区**

如果我们可以先生成唯一的 PhotoID，然后通过“PhotoID % 10”找到一个分片号，上述问题就迎刃而解了。在这种情况下，我们不需要在 ShardID 中附加 PhotoID，因为 PhotoID 本身在整个系统中都是唯一的。

**我们如何生成 PhotoID？**在这里，我们不能在每个分片中使用自动递增序列来定义 PhotoID，因为我们需要首先知道 PhotoID 才能找到将存储它的分片。一种解决方案可能是我们专用一个单独的数据库实例来生成自动递增的 ID。如果我们的 PhotoID 可以容纳 64 位，我们可以定义一个只包含 64 位 ID 字段的表。所以每当我们想在我们的系统中添加一张照片时，我们可以在这个表中插入一个新行，并将那个 ID 作为我们新照片的 PhotoID。

**这个密钥生成数据库不会是单点故障吗？**是的，会的。一种解决方法是定义两个这样的数据库，其中一个生成偶数编号的 ID，另一个生成奇数编号。对于 MySQL，以下脚本可以定义这样的序列

```
KeyGeneratingServer1:
auto-increment-increment = 2
auto-increment-offset = 1
 
KeyGeneratingServer2:
auto-increment-increment = 2
auto-increment-offset = 2
```

我们可以在这两个数据库前面放置一个负载均衡器，以在它们之间进行循环并处理停机时间。这两个服务器可能不同步，其中一个生成的密钥比另一个多，但这不会在我们的系统中造成任何问题。我们可以通过为用户、照片评论或系统中存在的其他对象定义单独的 ID 表来扩展此设计。

**或者**，我们可以实现类似于我们在[设计像TinyURL这样的URL缩短服务](she-ji-xiang-tinyurl-zhe-yang-de-url-suo-duan-fu-wu.md)中讨论过的“密钥”生成方案

**我们如何规划系统的未来发展？** 我们可以拥有大量的逻辑分区来适应未来的数据增长，这样一开始，多个逻辑分区驻留在单个物理数据库服务器上。由于每个数据库服务器上可以有多个数据库实例，因此我们可以为任何服务器上的每个逻辑分区拥有单独的数据库。所以每当我们觉得某个特定的数据库服务器有很多数据时，我们就可以将它的一些逻辑分区迁移到另一台服务器上。我们可以维护一个配置文件（或单独的数据库），将我们的逻辑分区映射到数据库服务器；这将使我们能够轻松地移动分区。每当我们想要移动一个分区时，我们只需要更新配置文件来宣布更改。

### 11、排名和News Feed生成

要为任何给定用户创建News Feed，我们需要获取用户关注的人的最新、最受欢迎和相关的照片。

为简单起见，假设我们需要为用户的 News Feed 获取前 100 张照片。我们的应用服务器将首先获取用户关注的人员列表，然后从每个用户获取最新 100 张照片的元数据信息。在最后一步，服务器会将所有这些照片提交给我们的排名算法，该算法将确定前 100 张照片（基于新近度、相似度等）并将它们返回给用户。这种方法的一个可能问题是更高的延迟，因为我们必须查询多个表并对结果执行排序/合并/排名。为了提高效率，我们可以预先生成 News Feed 并将其存储在单独的表中。

**预先生成News Feed：**我们可以有专门的服务器来持续生成用户的新闻提要并将它们存储在“UserNewsFeed”表中。因此，当任何用户需要最新照片作为他们的新闻提要时，我们将简单地查询此表并将结果返回给用户。

每当这些服务器需要生成用户的新闻提要时，它们将首先查询 UserNewsFeed 表以查找上次为该用户生成新闻提要的时间。然后，将从那时起生成新的 News Feed 数据（按照上述步骤）。

**向用户发送新闻提要内容的不同方法是什么？**

1、**Pull：**客户端可以定期从服务器拉取新闻提要内容，也可以在需要时手动拉取。这种方法可能存在的问题是

* a) 在客户端发出拉取请求之前，新数据可能不会显示给用户
* b) 如果没有新数据，大多数时候拉取请求将导致空响应。

2、**Push：**服务器可以在新数据可用时立即将其推送给用户。为了有效地管理这一点，用户必须向服务器维护一个[Long Poll](https://en.wikipedia.org/wiki/Push\_technology#Long\_polling)请求以接收更新。这种方法的一个可能问题是，一个关注很多人的用户或一个拥有数百万粉丝的名人用户；在这种情况下，服务器必须非常频繁地推送更新。

3、**推拉结合：**我们可以采用混合方法。

我们可以将所有拥有大量关注的用户移动到基于Pull的模型中，并且只将数据Push给那些拥有数百（或数千）关注的用户。

另一种方法可能是服务器以不超过一定频率向所有用户推送更新，让具有大量关注/更新的用户定期提取数据。

有关新闻源生成的详细讨论，请查看[设计 Facebook 的新闻](https://www.educative.io/collection/page/5668639101419520/5649050225344512/5641332169113600)源。

### 12、使用分片数据创建新闻提要

为任何给定用户创建新闻提要的最重要要求之一是从用户关注的所有人那里获取最新照片。为此，我们需要一种机制来根据照片的创建时间对照片进行排序。为了有效地做到这一点，我们可以将照片创建时间作为 PhotoID 的一部分。由于我们将在 PhotoID 上有一个主索引，因此可以很快找到最新的 PhotoID。

我们可以为此使用纪元时间。假设我们的 PhotoID 将有两个部分；第一部分将代表纪元时间，第二部分将是一个自动递增的序列。因此，要创建一个新的 PhotoID，我们可以采用当前纪元时间并从我们的密钥生成数据库中附加一个自动递增的 ID。我们可以从这个 PhotoID (PhotoID % 10) 中找出分片号并将照片存储在那里。

**我们 PhotoID 的大小可能是多少**？假设我们的纪元时间从今天开始，我们需要多少位来存储未来 50 年的秒数？

```
86400 秒/天 * 365（一年中的天数）* 50（年）=> 16 亿秒
```

我们需要 31 位来存储这个数字。因为平均而言，我们预计每秒有 23 张新照片；我们可以分配 9 位来存储自动递增的序列。所以每秒我们可以存储 (2^9 => 512) 张新照片。我们可以每秒重置我们的自动递增序列。

我们将在[设计 Twitter](https://www.educative.io/collection/page/5668639101419520/5649050225344512/5741031244955648)中的“数据分片”下讨论有关此技术的更多详细信息。

### 13、缓存和负载均衡

我们的服务需要一个大规模的照片交付系统来服务于全球分布的用户。我们的服务应该使用大量地理分布的照片缓存服务器和使用 CDN 将其内容推送到更靠近用户的位置（有关详细信息，请参阅[缓存](https://www.educative.io/collection/page/5668639101419520/5649050225344512/5643440998055936/)）。

我们可以为元数据服务器引入缓存来缓存热数据库行。我们可以使用 Memcache 来缓存数据，应用服务器在访问数据库之前可以快速检查缓存是否有所需的行。最近最少使用（LRU）可能是我们系统的合理缓存驱逐策略。根据此政策，我们首先丢弃最近最少查看的行。

**我们如何构建更智能的缓存？**如果我们遵循 80-20 规则，即 20% 的照片每日阅读量产生 80% 的流量，这意味着某些照片非常受欢迎，以至于大多数人都会阅读它们。这表明我们可以尝试缓存 20% 的每日照片和元数据读取量。
