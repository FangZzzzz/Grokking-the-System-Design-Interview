# 设计 Twitter 搜索

## 设计 Twitter 搜索

Twitter 是最大的社交网络服务之一，用户可以在其中分享照片、新闻和基于文本的消息。

在本章中，我们将设计一个可以存储和搜索用户推文的服务。类似问题：推文搜索。

难度级别：中等



### 1、什么是 Twitter 搜索？

Twitter 用户可以随时更新他们的状态。每个状态（称为推文）都由纯文本组成，我们的目标是设计一个允许搜索所有用户推文的系统。



### 2、系统的要求和目标 <a href="#div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px2-requirements-and-goals-of-th" id="div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px2-requirements-and-goals-of-th"></a>

* 假设 Twitter 拥有 15 亿总用户，每天有 8 亿活跃用户。
* Twitter 平均每天收到 4 亿条推文。
* 一条推文的平均大小为 300 字节。
* 假设每天会有 5 亿次搜索。
* 搜索查询将由多个与 AND/OR 组合的单词组成。

我们需要设计一个可以高效存储和查询推文的系统。



### 3、容量估计和约束 <a href="#div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px3-capacity-estimation-and-cons" id="div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px3-capacity-estimation-and-cons"></a>

**存储容量：**由于我们每天有 4 亿条新推文，而每条推文平均为 300 字节，我们需要的总存储空间为：

```
400M * 300 => 120GB/天
```

每秒总存储量：

```
120GB / 24 小时 / 3600 秒 ~= 1.38MB/秒
```



### 4、系统API

我们可以使用 SOAP 或 REST API 来公开我们服务的功能；以下可能是搜索 API 的定义：

```
search(api_dev_key, search_terms, maximum_results_to_return, sort, page_token)
```

**参数：**\
api\_dev\_key(string)：注册账号的API开发者密钥。除其他外，这将用于根据分配的配额限制用户。\
search\_terms(string)：包含搜索词的字符串。\
maximum\_results\_to\_return (number): 要返回的推文数。\
排序（number）：可选排序模式：最新优先（0 - 默认）、最佳匹配 (1)、最喜欢 (2)。\
page\_token（string）：此令牌将指定结果集中应返回的页面。

**返回：** (JSON)\
一个 JSON，其中包含有关与搜索查询匹配的推文列表的信息。每个结果条目可以有用户 ID 和名称、推文文本、推文 ID、创建时间、点赞数等。



### 5、高级设计

在高层次上，我们需要将所有状态存储在数据库中，并建立一个索引来跟踪哪个单词出现在哪个推文中。该索引将帮助我们快速找到用户尝试搜索的推文。

<figure><img src="../.gitbook/assets/image (7) (3).png" alt=""><figcaption><p>Twitter搜索的高级设计</p></figcaption></figure>



### 6、详细的组件设计

#### **1、存储**

我们每天需要存储120GB的新数据。鉴于如此庞大的数据量，我们需要提出一种数据分区方案，该方案将有效地将数据分布到多个服务器上。如果我们计划未来五年，我们将需要以下存储：

```
120GB * 365 天 * 5 年 ~= 200TB
```

如果我们在任何时候都不想超过 80%，我们大约需要 250TB 的总存储空间。假设我们想要保留所有推文的额外副本以实现容错；那么，我们的总存储需求将是 500TB。如果我们假设现代服务器可以存储多达 4TB 的数据，我们将需要 125 台这样的服务器来保存未来五年所需的所有数据。

让我们从一个简单的设计开始，我们将推文存储在 MySQL 数据库中。我们可以假设我们将推文存储在具有两列 TweetID 和 TweetText 的表中。假设我们根据 TweetID 对数据进行分区。如果我们的 TweetID 在系统范围内是唯一的，我们可以定义一个哈希函数，该函数可以将 TweetID 映射到可以存储该推文对象的存储服务器。

**我们如何创建系统范围内的唯一 TweetID？**如果我们每天收到 4 亿条新推文，那么五年内我们可以期待多少推文对象？

```
400M * 365 天 * 5 年 => 7300 亿
```

这意味着我们需要一个五个字节的数字来唯一标识 TweetID。假设我们有一个服务可以在我们需要存储一个对象时生成一个唯一的 TweetID（这里讨论的 TweetID 将类似于[设计 Twitter](she-ji-twitter.md#div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px7-data-sharding)中讨论的 TweetID ）。我们可以将 TweetID 提供给我们的哈希函数以找到存储服务器并将我们的推文对象存储在那里。

#### 2、**索引**

我们的索引应该是什么样的？由于我们的推文查询将由单词组成，因此让我们构建一个索引，该索引可以告诉我们哪个单词来自哪个推文对象。让我们先估计一下我们的索引会有多大。如果我们想为所有的英文单词和一些著名的名词（如人名、城市名称等）建立一个索引，并且假设我们有大约 300K 的英文单词和 200K 的名词，那么我们的单词总数将有 500k指数。假设一个单词的平均长度是五个字符。如果我们将索引保存在内存中，我们需要 2.5MB 的内存来存储所有单词**：**

```
500K * 5 => 2.5 MB
```

假设我们希望将索引保存在内存中，以存储仅过去两年的所有推文。由于我们将在 5 年内收到 730亿条推文，这将在两年内为我们提供 292亿条推文。假设每个 TweetID 为 5 个字节，我们需要多少内存来存储所有 TweetID？

```
292亿 * 5 => 1460 GB
```

所以我们的索引就像一个大的分布式哈希表，其中“key”是单词，“value”是包含该单词的所有推文的 TweetID 列表。假设平均每条推文中有 40 个词，并且由于我们不会索引介词和其他小词，如“the”、“an”、“and”等，假设每条推文中大约有 15 个词需要被索引。这意味着每个 TweetID 将在我们的索引中存储 15 次。所以我们需要存储索引的总内存：

```
(1460 * 15) + 2.5MB ~= 21 TB
```

假设一个高端服务器有 144GB 内存，我们需要 152 个这样的服务器来保存我们的索引。

我们可以根据两个标准对数据进行分区：

**基于单词的分片：** 在构建索引时，我们将遍历推文的所有单词并计算每个单词的哈希值以找到将被索引的服务器。要查找包含特定单词的所有推文，我们必须仅查询包含该单词的服务器。

这种方法有几个问题：

1. 如果一个词变得炙手可热怎么办？然后在服务器上会有很多查询持有那个词。这种高负载会影响我们服务的性能。
2. 随着时间的推移，与其他词相比，某些词最终会存储大量 TweetID，因此，在推文增长的同时保持词的均匀分布是相当棘手的。

要从这些情况中恢复，我们要么必须重新分区我们的数据，要么使用[Consistent Hashing](https://www.educative.io/collection/page/5668639101419520/5649050225344512/5709068098338816/)。

**基于推文对​​象的分片：**在存储时，我们会将 TweetID 传递给我们的哈希函数以查找服务器并索引该服务器上推文的所有单词。在查询特定单词时，我们必须查询所有服务器，每个服务器都会返回一组 TweetID。中央服务器将汇总这些结果以将它们返回给用户。

<figure><img src="../.gitbook/assets/image (3).png" alt=""><figcaption><p>详细的组件设计</p></figcaption></figure>

### 7、容错

当索引服务器死机时会发生什么？我们可以拥有每台服务器的辅助副本，如果主服务器死机，它可以在故障转移后获得控制权。主服务器和辅助服务器都将具有相同的索引副本。

如果主服务器和辅助服务器同时死机怎么办？我们必须分配一个新服务器并在其上重建相同的索引。我们怎么能做到这一点？我们不知道该服务器上保存了哪些文字/推文。如果我们使用“基于推文对​​象的分片”，暴力解决方案将是遍历整个数据库并使用我们的哈希函数过滤 TweetID，以找出将存储在该服务器上的所有必需推文。这将是低效的，而且在重建服务器期间，我们将无法从中提供任何查询，因此会丢失一些用户应该看到的推文。

我们如何有效地检索推文和索引服务器之间的映射？我们必须建立一个反向索引，将所有 TweetID 映射到他们的索引服务器。我们的 Index-Builder 服务器可以保存这些信息。我们需要构建一个 Hashtable，其中“key”将是索引服务器编号，“value”将是一个 HashSet，其中包含保存在该索引服务器上的所有 TweetID。请注意，我们将所有 TweetID 保存在 HashSet 中；这将使我们能够快速从索引中添加/删除推文。所以现在，每当索引服务器必须重建自身时，它可以简单地向 Index-Builder 服务器询问它需要存储的所有推文，然后获取这些推文来构建索引。这种方法肯定会很快。我们还应该有一个 Index-Builder 服务器的副本以实现容错。

### 8、缓存

为了处理热门推文，我们可以在数据库前面引入缓存。我们可以使用[Memcached](https://en.wikipedia.org/wiki/Memcached)，它可以将所有此类热门推文存储在内存中。应用程序服务器在访问后端数据库之前，可以快速检查缓存中是否有该推文。根据客户的使用模式，我们可以调整我们需要多少缓存服务器。对于缓存驱逐策略，最近最少使用 (LRU) 似乎适合我们的系统。

### 9.负载均衡

我们可以在系统中的两个地方添加负载均衡层：

1）在客户端和应用程序服务器之间以及&#x20;

2）在应用程序服务器和后端服务器之间。

最初，可以采用简单的循环方法；在后端服务器之间平均分配传入请求。这个 LB 实现简单，不会引入任何开销。这种方法的另一个好处是 LB 将使死服务器退出轮换，并停止向其发送任何流量。Round Robin LB 的一个问题是它不会考虑服务器负载。如果服务器过载或速度慢，LB 将不会停止向该服务器发送新请求。为了解决这个问题，可以放置一个更智能的 LB 解决方案，它会定期向后端服务器查询其负载并基于此调整流量。

### 10、排名

如果我们想按社交图距离、流行度、相关性等对搜索结果进行排名呢？

假设我们想要按受欢迎程度对推文进行排名，例如一条推文获得了多少赞或评论等。在这种情况下，我们的排名算法可以计算一个“受欢迎程度”（基于赞的数量等）和将其与索引一起存储。在将结果返回到聚合器服务器之前，每个分区都可以根据此受欢迎程度对结果进行排序。聚合器服务器组合所有这些结果，根据受欢迎程度对它们进行排序，并将排名靠前的结果发送给用户。
