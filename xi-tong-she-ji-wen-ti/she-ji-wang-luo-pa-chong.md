# 设计网络爬虫

## 设计网络爬虫

让我们设计一个可以系统地浏览和下载万维网的网络爬虫。

网络爬虫也称为 web spiders，robots，worms，walkers，bots。

难度等级：难



### 1、什么是网络爬虫？

网络爬虫是一种以系统和自动化方式浏览万维网的软件程序。它通过从一组起始页面递归获取链接来收集文档。许多网站，尤其是搜索引擎，使用网络爬虫作为提供最新数据的一种方式。搜索引擎下载所有页面以在其上创建索引以执行更快的搜索。

网络爬虫的其他一些用途是：

* 测试网页和链接的有效语法和结构。
* 监视站点以查看其结构或内容何时发生变化。
* 维护流行网站的镜像站点。
* 搜索版权侵权。
* 建立一个专用索引，例如，一个对存储在 Web 上的多媒体文件中的内容有一定了解的索引。

### 2、系统的要求和目标

假设我们需要爬取所有的网络。

**可扩展性：**我们的服务需要具有可扩展性，以便它可以爬取整个 Web 并可以用于获取数亿个 Web 文档。

**可扩展性：**我们的服务应该以模块化的方式设计，并期望添加新的功能。将来可能需要下载和处理更新的文档类型。

### 3、一些设计考虑

爬网是一项复杂的任务，有很多方法可以解决。在继续之前，我们应该问几个问题：

**它只是 HTML 页面的爬虫吗？或者我们应该获取和存储其他类型的媒体，例如声音文件、图像、视频等？**这很重要，因为答案可以改变设计。如果我们正在编写一个通用爬虫来下载不同的媒体类型，我们可能希望将解析模块分解为不同的模块集：一个用于 HTML，另一个用于图像，或者另一个用于视频，其中每个模块提取所考虑的内容对那种媒体类型很有趣。

现在让我们假设我们的爬虫将只处理 HTML，但它应该是可扩展的，并且可以很容易地添加对新媒体类型的支持。

**我们在看什么协议？HTTP？FTP链接呢？我们的爬虫应该处理哪些不同的协议？**为了练习，我们将假设 HTTP。同样，以后扩展设计以使用 FTP 和其他协议应该不难。

**我们将抓取的预期页面数是多少？URL 数据库将有多大？** 假设我们需要抓取十亿个网站。由于一个网站可以包含很多很多的 URL，让我们假设我们的爬虫可以访问 150 亿个不同网页的上限。

**什么是“RobotsExclusion”，我们应该如何处理？**礼貌的网络爬虫实施了机器人排除协议，该协议允许网站管理员声明其网站的某些部分不受爬虫的限制。机器人排除协议要求网络爬虫在从网站下载任何真实内容之前，从网站获取一个名为 robots.txt 的特殊文档，其中包含这些声明。



### 4、容量估计和约束

如果我们想在 4 周内爬取 150 亿个页面，我们需要每秒抓取多少个页面？

```
15B /（4 周 * 7 天 * 86400 秒）~= 6200 页/秒
```

**存储呢？**页面大小变化很大，但是，如上所述，因为我们将只处理 HTML 文本，让我们假设平均页面大小为 100KB。对于每一页，如果我们存储 500 字节的元数据，我们需要的总存储空间：

```
15B * (100KB + 500) ~= 1.5 PB
```

假设一个 70% 容量模型（我们不想超过存储系统总容量的 70%），我们需要的总存储空间：

```
1.5 PB / 0.7 ~= 2.14 PB
```

### 5、高层次设计

任何网络爬虫执行的基本算法都是以种子 URL 列表作为其输入，并重复执行以下步骤。

1. 从未访问的 URL 列表中选择一个 URL。
2. 确定其主机名的 IP 地址。
3. 与主机建立连接以下载相应的文档。
4. 解析文档内容以查找新的 URL。
5. 将新 URL 添加到未访问的 URL 列表中。
6. 处理下载的文档，例如存储或索引其内容等。
7. 返回步骤 1

#### 怎么爬？

**广度优先还是深度优先？**通常使用广度优先搜索（BFS）。但是，在某些情况下也会使用深度优先搜索 (DFS)，例如，如果您的爬虫已经与网站建立了连接，它可能只是对网站内的所有 URL 进行 DFS，以节省一些握手开销。

**路径升序爬取：**路径升序爬取可以帮助发现许多孤立的资源或在常规爬取特定网站时找不到入站链接的资源。在此方案中，爬虫将上升到它打算爬取的每个 URL 中的每个路径。例如，当给定一个种子 URL [http://foo.com/a/b/page.html](http://foo.com/a/b/page.html)时，它将尝试爬取 /a/b/、/a/ 和 /。

#### 实现高效网络爬虫的难点 <a href="#difficulties-in-implementing-efficient-web-crawler" id="difficulties-in-implementing-efficient-web-crawler"></a>

Web 的两个重要特征使 Web 爬网成为一项非常困难的任务：

**1. 大量网页：**大量网页意味着网络爬虫在任何时候只能下载一小部分网页，因此网络爬虫必须足够智能以优先下载。

**2. 网页的变化率。**当今动态世界的另一个问题是互联网上的网页变化非常频繁。因此，当爬虫从站点下载最后一个页面时，页面可能会发生变化，或者可能会向站点添加新页面。

一个最低限度的爬虫至少需要这些组件：

**1. URL frontier：**存储要下载的 URL 列表，并优先考虑哪些 URL 应该首先被爬取。\
**2. HTTP Fetcher：**从服务器检索网页。\
**3. Extractor：**从 HTML 文档中提取链接。\
**4. Duplicate Eliminator：**确保相同的内容不会被无意中提取两次。\
**5. Datastore：**存储检索到的页面、URL 和其他元数据。

\


<figure><img src="../.gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>

### 6、详细的组件设计

假设我们的爬虫在一台服务器上运行，所有的爬取都是由多个工作线程完成的，每个工作线程执行循环下载和处理文档所需的所有步骤。

这个循环的第一步是将一个绝对的URL从共享的URL边界中移除，以供下载。一个绝对的URL以一个方案开始（例如，"HTTP"），该方案确定了应该用来下载的网络协议。我们可以用模块化的方式来实现这些协议的可扩展性，这样以后如果我们的爬虫需要支持更多的协议，就可以很容易地完成。根据URL的方案，工作者调用适当的协议模块来下载文档。下载后，文档被放入一个文档输入流（DIS）。将文档放入DIS将使其他模块能够多次重读该文档。

将文档写入 DIS 后，工作线程调用重复数据移除测试以确定该文档（与不同的 URL 相关联）是否曾被查看过。如果是这样，则不会进一步处理文档，并且工作线程会从边界中移除下一个 URL。

接下来，我们的爬虫需要处理下载的文档。每个文档可以有不同的 MIME 类型，如 HTML 页面、图像、视频等。我们可以以模块化的方式实现这些 MIME 方案，以便以后如果我们的爬虫需要支持更多类型，我们可以轻松实现它们。根据下载文档的 MIME 类型，worker 调用与该 MIME 类型关联的每个处理模块的 process 方法。

此外，我们的 HTML 处理模块将从页面中提取所有链接。每个链接都被转换为绝对 URL，并针对用户提供的 URL 过滤器进行测试，以确定是否应该下载它。如果 URL 通过过滤器，worker 执行 URL-seen 测试，检查该 URL 之前是否已被看到，即它是否在 URL 边界或已经下载。如果 URL 是新的，则将其添加到边界。

\
\


<figure><img src="../.gitbook/assets/image.png" alt=""><figcaption></figcaption></figure>

让我们一一讨论这些组件，看看它们如何分布到多台机器上：

**1. The URL frontier：**URL 边界是包含所有待下载的 URL 的数据结构。我们可以通过执行 Web 的广度优先遍历来爬取，从种子集中的页面开始。这种遍历很容易通过使用 FIFO 队列来实现。

由于我们将拥有大量要抓取的 URL，因此我们可以将 URL 边界分布到多个服务器中。假设在每台服务器上我们有多个工作线程执行爬取任务。我们还假设我们的哈希函数将每个 URL 映射到一个负责抓取它的服务器。

在设计分布式 URL 边界时，必须牢记以下礼貌要求：

1. 我们的爬虫不应该通过从服务器下载大量页面来使服务器过载。
2. 我们不应该有多台机器连接一个 Web 服务器。

为了实现这种礼貌约束，我们的爬虫可以在每个服务器上拥有一组不同的 FIFO 子队列。每个工作线程都有其单独的子队列，从中删除用于抓取的 URL。当需要添加一个新的 URL 时，它所在的 FIFO 子队列将由 URL 的规范主机名决定。我们的哈希函数可以将每个主机名映射到一个线程号。这两点一起意味着，一个工作线程最多可以从给定的 Web 服务器下载文档，而且，通过使用 FIFO 队列，它不会使 Web 服务器过载。

**我们的 URL 边界会有多大？**大小将是数亿个 URL。因此，我们需要将 URL 存储在磁盘上。我们可以以这样一种方式实现我们的队列，即它们具有用于入队和出队的单独缓冲区。入队缓冲区一旦被填满，将被转储到磁盘，而出队缓冲区将保留需要访问的 URL 的缓存；它可以定期从磁盘读取以填充缓冲区。

**2. fetcher 模块：fetcher 模块**的目的是使用适当的网络协议（如 HTTP）下载与给定 URL 对应的文档。如上所述，网站管理员创建robot.txt 以使其网站的某些部分不受爬虫的限制。为避免每次请求都下载此文件，我们的爬虫的 HTTP 协议模块可以维护一个固定大小的缓存，将主机名映射到其机器人的排除规则。

**3. Document input stream：**我们的爬虫设计使得同一个文档可以被多个处理模块处理。为了避免多次下载文档，我们使用称为文档输入流 (DIS) 的抽象在本地缓存文档。

DIS 是一个输入流，它缓存从 Internet 读取的文档的全部内容。它还提供了重新阅读文档的方法。DIS 可以将小文档（64 KB 或更少）完全缓存在内存中，而较大的文档可以临时写入支持文件。

每个工作线程都有一个关联的 DIS，它在文档之间重用它。从边界提取 URL 后，worker 将该 URL 传递给相关的协议模块，该模块从网络连接初始化 DIS 以包含文档的内容。然后工作人员将 DIS 传递给所有相关的处理模块。

**4. 文档重复数据删除测试：** Web 上的许多文档都可以在多个不同的 URL 下使用。也有很多情况下，文档被镜像到不同的服务器上。这两种影响都会导致任何网络爬虫多次下载同一个文档。为了防止对文档进行多次处理，我们对每个文档执行重复数据删除测试以消除重复。

为了执行这个测试，我们可以计算每个已处理文档的 64 位校验和并将其存储在数据库中。对于每个新文档，我们可以将其校验和与之前计算的所有校验和进行比较，以查看该文档之前是否已查看过。我们可以使用 MD5 或 SHA 来计算这些校验和。

**校验和存储有多大？** 如果我们校验和存储的全部目的是进行重复数据删除，那么我们只需要保留一个唯一的集合，其中包含所有先前处理的文档的校验和。考虑到 150 亿个不同的网页，我们需要：

15B \* 8 字节 => 120 GB

尽管这可以适应现代服务器的内存，但如果我们没有足够的可用内存，我们可以在每台服务器上保留较小的基于 LRU 的缓存，所有内容都由持久存储支持。重复数据删除测试首先检查缓存中是否存在校验和。如果不是，它必须检查校验和是否驻留在后端存储中。如果找到校验和，我们将忽略该文档。否则，它将被添加到缓存和后台存储中。

**5. URL filter：** URL 过滤机制提供了一种可定制的方式来控制下载的 URL 集。这用于将网站列入黑名单，以便我们的爬虫可以忽略它们。在将每个 URL 添加到边界之前，工作线程会咨询用户提供的 URL 过滤器。我们可以定义过滤器以按域、前缀或协议类型限制 URL。

**6.域名解析：**在联系Web服务器之前，Web爬虫必须使用域名服务（DNS）将Web服务器的主机名映射到IP地址。鉴于我们将使用的 URL 数量，DNS 名称解析将成为我们爬虫的一大瓶颈。为了避免重复请求，我们可以通过构建本地 DNS 服务器来开始缓存 DNS 结果。

**7. URL去重测试：**在提取链接时，任何网络爬虫都会遇到同一个文档的多个链接。为避免多次下载和处理文档，在将提取的每个链接添加到 URL 边界之前，必须对其执行 URL 重复数据删除测试。

为了执行 URL 重复数据删除测试，我们可以将爬虫看到的所有 URL 以规范的形式存储在数据库中。为了节省空间，我们不在 URL 集中存储每个 URL 的文本表示，而是存储一个固定大小的校验和。

为了减少对数据库存储的操作数量，我们可以在所有线程共享的每个主机上保留一个流行 URL 的内存缓存。有这个缓存的原因是一些 URL 的链接很常见，所以在内存中缓存流行的 URL 会导致内存中的高命中率。

**URL 的存储需要多少存储空间？**如果我们校验和的全部目的是进行 URL 重复数据删除，那么我们只需要保留一个唯一的集合，其中包含所有以前看到的 URL 的校验和。考虑到 150 亿个不同的 URL 和 4 个字节的校验和，我们需要：

```
15B * 4 字节 => 60 GB
```

**我们可以使用布隆过滤器进行重复数据删除吗？** 布隆过滤器是一种概率数据结构，用于可能产生误报的集合成员测试。一个大的位向量表示该集合。通过计算元素的“n”个散列函数并设置相应的位，将元素添加到集合中。如果设置了元素哈希位置的所有“n”位，则认为该元素在集合中。因此，文档可能被错误地认为在集合中，但不可能出现假阴性。

对 URL 可见测试使用布隆过滤器的缺点是每个误报都会导致 URL 不被添加到边界，因此永远不会下载文档。可以通过增大位向量来减少误报的机会。

**8. 检查点：**整个 Web 的爬网需要数周时间才能完成。为了防止失败，我们的爬虫可以将其状态的定期快照写入磁盘。可以轻松地从最新的检查点重新启动中断或中止的爬网。

### 7、容错

我们应该使用一致的散列在爬网服务器之间进行分配。一致的散列不仅有助于替换死主机，还有助于在爬网服务器之间分配负载。

我们所有的爬虫服务器都将执行定期检查点并将其 FIFO 队列存储到磁盘中。如果服务器出现故障，我们可以更换它。同时，一致性哈希应该将负载转移到其他服务器。

### 8、数据分区

我们的爬虫将处理三种数据：

1）要访问的 URL&#x20;

2）重复数据删除的 URL 校验和&#x20;

3）重复数据删除的文档校验和。

由于我们基于主机名分发 URL，因此我们可以将这些数据存储在同一主机上。因此，每个主机将存储其需要访问的 URL 集、所有先前访问的 URL 的校验和以及所有下载文档的校验和。由于我们将使用一致的散列，我们可以假设 URL 将从过载的主机重新分配。

每个主机将定期执行检查点并将其持有的所有数据的快照转储到远程服务器上。这将确保如果一台服务器宕机，另一台服务器可以通过从上次快照中获取其数据来替换它。



### 9、爬虫陷阱

有许多爬虫陷阱、垃圾邮件站点和隐藏内容。爬虫陷阱是导致爬虫无限期爬取的一个 URL 或一组 URL。一些爬虫陷阱是无意的。例如，文件系统中的符号链接可以创建一个循环。其他爬虫陷阱是有意引入的。例如，人们编写了动态生成无限网络文档的陷阱。这些陷阱背后的动机各不相同。反垃圾邮件陷阱旨在捕获垃圾邮件发送者使用的搜寻电子邮件地址的爬虫，而其他网站则使用陷阱捕获搜索引擎爬虫以提高其搜索评级。

\
